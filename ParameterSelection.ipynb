{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, size_list, dropout = False, dropoutProb = 0.1, batchNorm = False):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        self.size_list = size_list\n",
    "        for i in range(len(size_list) - 2):\n",
    "            layers.append(nn.Linear(size_list[i],size_list[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "            \n",
    "            if batchNorm:\n",
    "                layers.append(nn.BatchNorm1d(size_list[i+1]))\n",
    "                \n",
    "            if dropout:\n",
    "                layers.append(nn.Dropout(p = dropoutProb))\n",
    "            \n",
    "        layers.append(nn.Linear(size_list[-2], size_list[-1]))\n",
    "        \n",
    "        # Unpack the list\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, size_list):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.LSTM(         # if use nn.RNN(), it hardly learns\n",
    "            input_size=size_list[0],\n",
    "            hidden_size=size_list[1],         # rnn hidden unit\n",
    "            num_layers=len(size_list)-2,           # number of rnn layer\n",
    "            )\n",
    "\n",
    "        self.out = nn.Linear(size_list[-2], size_list[-1])\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        r_out, (h_n, h_c) = self.rnn(x, None)   # None represents zero initial hidden state\n",
    "\n",
    "        # choose r_out at the last time step\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "\n",
    "def train_epoch(model, optimizer, X_train, y_train, criterion):\n",
    "    model.train()\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    optimizer.zero_grad()  \n",
    "\n",
    "    outputs = model(X_train)\n",
    "    \n",
    "    loss = criterion(outputs, y_train)\n",
    "    \n",
    "    running_loss = loss.item()\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    #print('Training Loss: ', running_loss, 'Time: ',end_time - start_time, 's')\n",
    "    return running_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Optimizer can be 'SGD', 'RMSprop', 'ADAM', \n",
    "def nnTrain(X_train, y_train, size_list, dropout = False, dropoutProb = 0.1, batchNorm = False, \n",
    "                optimizer = 'SGD', lr = 0.01, n_epochs = 100, LSTM = False):   \n",
    "\n",
    "    X_train = torch.autograd.Variable(torch.Tensor(X_train.values.astype(float)))\n",
    "    y_train = torch.autograd.Variable(torch.Tensor(y_train.values.astype(float)))\n",
    "    \n",
    "    if LSTM == True:\n",
    "        X_train = X_train.view(-1, X_train.shape[0], X_train.shape[1])\n",
    "        model = RNN(size_list)\n",
    "    else:\n",
    "        model = MLP(size_list, dropout, dropoutProb, batchNorm)\n",
    "    \n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    if optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr = lr)\n",
    "    if optimizer == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr = lr)\n",
    "    if optimizer == 'ADAM':\n",
    "        optimizer = optim.ADAM(model.parameters(), lr = lr)\n",
    "    \n",
    "    Train_loss = []\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        train_loss = train_epoch(model, optimizer, X_train, y_train, criterion)\n",
    "        Train_loss.append(train_loss)\n",
    "    \n",
    "    return model, Train_loss\n",
    "\n",
    "\n",
    "def nnTest(model, X_test, y_test):\n",
    "    X_test = torch.autograd.Variable(torch.Tensor(X_test.values.astype(float)))\n",
    "    y_test = torch.autograd.Variable(torch.Tensor(y_test.values.astype(float)))\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    if hasattr(model, 'rnn'):\n",
    "        X_test = X_test.view(-1, X_test.shape[0], X_test.shape[1])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()        \n",
    "\n",
    "        outputs = model(X_test)\n",
    "\n",
    "        loss = criterion(outputs, y_test).detach()\n",
    "        running_loss = loss.item()\n",
    "        \n",
    "        return running_loss\n",
    "    \n",
    "def nnPredict(model, X_test):\n",
    "    X_test = torch.autograd.Variable(torch.Tensor(X_test.values.astype(float)))\n",
    "    \n",
    "    if hasattr(model, 'rnn'):\n",
    "        X_test = X_test.view(-1, X_test.shape[0], X_test.shape[1])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()        \n",
    "        outputs = model(X_test)\n",
    "    return np.array(outputs).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import pandas_datareader as web\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import neural_network as npy\n",
    "import os \n",
    "import sys\n",
    "\n",
    "\n",
    "def get_random_tickers(n, ticklist):\n",
    "    if n >len(ticklist):\n",
    "        raise Exception('n is bigger than ticklist')\n",
    "    return np.random.choice(ticklist, size=n, replace=False)\n",
    "RSVE = []\n",
    "\n",
    "\n",
    "# Our initial ticklist\n",
    "ticklist = ['ADS.DE', 'ALV.DE', 'BAS.DE', 'BEI.DE', 'BMW.DE', 'CON.DE', 'DAI.DE', 'DBK.DE', 'DTE.DE', 'EOAN.DE', 'FME.DE',\n",
    " 'FRE.DE', 'HEI.DE', 'HEN3.DE', 'LHA.DE', 'LIN.DE', 'MRK.DE', 'MUV2.DE', 'RWE.DE', 'SAP.DE', 'SIE.DE', 'TKA.DE', 'VOW3.DE']\n",
    "\n",
    "# The 7 tickers chosen randomly from this ticklist\n",
    "\n",
    "ticklist = get_random_tickers(10, ticklist)\n",
    "#ticklist = ['LIN.DE', 'HEN3.DE', 'DAI.DE', 'FME.DE', 'MUV2.DE', 'SIE.DE', 'DBK.DE']\n",
    "ticklist = ['CON.DE', 'RWE.DE', 'DTE.DE', 'BEI.DE', 'HEI.DE', 'LIN.DE','FRE.DE', 'ADS.DE','FME.DE','MRK.DE']\n",
    "\n",
    "tickdict = dict(zip(ticklist, range(1,len(ticklist)+1)))\n",
    "\n",
    "## Paramteres\n",
    "d0 = '2001-01-01' # begining of the waiting period\n",
    "d1 = '2004-01-01' # end of the CV period - beg\n",
    "d2 = '2006-01-01' # begining of the test period\n",
    "d3 = '2008-01-01' # end of the test period\n",
    "\n",
    "dcv1 = '2004-01-01'\n",
    "dcv2 = '2005-01-01'\n",
    "\n",
    "\n",
    "### Parameters\n",
    "norm=True\n",
    "cv_nlayers=True\n",
    "cv_nneurones = False\n",
    "lbd = 0.2 # history weight metric\n",
    "alpha = 0.09 # risk management metric\n",
    "cst = 10 #weight metric\n",
    "delta = 0.91\n",
    "\n",
    "\n",
    "\n",
    "nneurones = 10\n",
    "nlayers = 4\n",
    "drp = False\n",
    "drpProb = 0.1\n",
    "batch = False \n",
    "opt = 'SGD'\n",
    "learning_rate = 0.01\n",
    "n_epochs = 400\n",
    "\n",
    "\n",
    "\n",
    "# Generate the X matrix and Y matrix and make them have only trading days\n",
    "\n",
    "dtes = pd.read_csv('trading_days.csv', index_col=0)\n",
    "tempdt = dtes.copy()\n",
    "tempdt.set_index('Buy', drop=True, inplace=True)\n",
    "tempdt.index = pd.to_datetime(tempdt.index)\n",
    "\n",
    "Y = []\n",
    "X = pd.DataFrame(columns=['Date', 'EMA10', 'EMA16', 'EMA22', 'SMA10', 'SMA16', 'SMA22','ValueAtRisk', \n",
    "                       'Bollu20', 'Bollu26', 'Bollu32', 'Bolld20', 'Bolld26', 'Bolld32',\n",
    "                       'Mom12', 'Mom18', 'Mom24', 'ACC12', 'ACC18', 'ACC24', 'ROC10', 'ROC16',\n",
    "                       'ROC22', 'MACD1812', 'MACD2412','MACD3012', 'MACDS18129', 'MACDS24129', 'MACDS30129', \n",
    "                       'RSI8', 'RSI14', 'RSI20', 'OBV', 'CHV1010', 'CHV1016', 'CHV1022',\n",
    "                       'FastK12', 'FastD12', 'FastK18', 'SlowK12', 'FastD18', 'SlowD12',\n",
    "                       'FastK24', 'SlowK18', 'FastD24', 'SlowD18', 'SlowK24', 'SlowD24',\n",
    "                       'High', 'Low', 'Open', 'Close', 'Volume', 'AdjClose', \n",
    "                       'Ticker','Month', 'DAX', 'ADL', 'Type1', 'Type2', 'Type3', 'Y'])\n",
    "\n",
    "dax = web.get_data_yahoo('^GDAXI', start=d0, end=d3)\n",
    "prices = pd.DataFrame(columns=['Ticker', 'High', 'Low', 'Open', 'Close', 'Volume', 'AdjClose', 'Date'])\n",
    "\n",
    "\n",
    "for tick in ticklist:\n",
    "    if norm:\n",
    "        temp = pd.read_csv('tickDataNorm/'+ str(delta).replace('.', '') +'/' + tick.replace('.', '') +'.csv', index_col=0)\n",
    "    else:\n",
    "        temp = pd.read_csv('tickData/'+ str(delta).replace('.', '') +'/' + tick.replace('.', '') +'.csv', index_col=0)\n",
    "    prices = pd.concat([prices, temp.loc[:, ['Ticker', 'High', 'Low', 'Open', 'Close', 'Volume', 'AdjClose', 'Date']]],axis=0, ignore_index=True, sort=False,copy=True)\n",
    "    # Select dates around events\n",
    "    temp.set_index('Date', inplace=True, drop=True)\n",
    "    temp.index = pd.to_datetime(temp.index)\n",
    "    B = temp.loc[pd.to_datetime(dtes.Buy), 'Close']\n",
    "    S = temp.loc[pd.to_datetime(dtes.Sell), 'Close']\n",
    "    temp = temp.loc[tempdt.index, :]\n",
    "    temp['Y'] = 100*(S.values-B.values)/B.values\n",
    "    \n",
    "    mask = np.logical_not(np.isnan(temp['Y'].values))\n",
    "    temp = temp.loc[mask, :]\n",
    "    if norm:\n",
    "        temp.loc[:,'High'] = temp.loc[:,'Norm_High']\n",
    "        temp.loc[:,'Low'] = temp.loc[:,'Norm_Low']\n",
    "        temp.loc[:,'Open'] = temp.loc[:,'Norm_Open']\n",
    "        temp.loc[:,'Close'] = temp.loc[:,'Norm_Close']\n",
    "        temp.loc[:,'AdjClose'] = temp.loc[:,'Norm_AdjClose']\n",
    "    #temp = temp.loc[pd.to_datetime(dtes.Buy), :]\n",
    "    temp['Month'] = temp.index.month\n",
    "    temp['Date'] = temp.index\n",
    "    temp['DAX'] = dax.loc[temp.index, 'Adj Close']\n",
    "    temp.loc[:,'Type'] = tempdt.loc[mask, 'Type'].values\n",
    "    temp = temp.loc[:,['Date', 'EMA10', 'EMA16', 'EMA22', 'SMA10', 'SMA16', 'SMA22','ValueAtRisk', \n",
    "                       'Bollu20', 'Bollu26', 'Bollu32', 'Bolld20', 'Bolld26', 'Bolld32',\n",
    "                       'Mom12', 'Mom18', 'Mom24', 'ACC12', 'ACC18', 'ACC24', 'ROC10', 'ROC16',\n",
    "                       'ROC22', 'MACD1812', 'MACD2412','MACD3012', 'MACDS18129', 'MACDS24129', 'MACDS30129', \n",
    "                       'RSI8', 'RSI14', 'RSI20', 'OBV', 'CHV1010', 'CHV1016', 'CHV1022',\n",
    "                       'FastK12', 'FastD12', 'FastK18', 'SlowK12', 'FastD18', 'SlowD12',\n",
    "                       'FastK24', 'SlowK18', 'FastD24', 'SlowD18', 'SlowK24', 'SlowD24',\n",
    "                       'High', 'Low', 'Open', 'Close', 'Volume', 'AdjClose', 'Ticker',\n",
    "                       'Month', 'DAX', 'ADL','Type', 'Y']]\n",
    "    \n",
    "\n",
    "    temp['Type1'] = (temp.loc[:,'Type'].values == 1)*1\n",
    "    temp['Type2'] = (temp.loc[:,'Type'].values == 2)*1\n",
    "    temp['Type3'] = (temp.loc[:,'Type'].values == 3)*1\n",
    "    temp = temp.loc[:,['Date', 'EMA10', 'EMA16', 'EMA22', 'SMA10', 'SMA16', 'SMA22','ValueAtRisk', \n",
    "                       'Bollu20', 'Bollu26', 'Bollu32', 'Bolld20', 'Bolld26', 'Bolld32',\n",
    "                       'Mom12', 'Mom18', 'Mom24', 'ACC12', 'ACC18', 'ACC24', 'ROC10', 'ROC16',\n",
    "                       'ROC22', 'MACD1812', 'MACD2412','MACD3012', 'MACDS18129', 'MACDS24129', 'MACDS30129', \n",
    "                       'RSI8', 'RSI14', 'RSI20', 'OBV', 'CHV1010', 'CHV1016', 'CHV1022',\n",
    "                       'FastK12', 'FastD12', 'FastK18', 'SlowK12', 'FastD18', 'SlowD12',\n",
    "                       'FastK24', 'SlowK18', 'FastD24', 'SlowD18', 'SlowK24', 'SlowD24',\n",
    "                       'High', 'Low', 'Open', 'Close', 'Volume', 'AdjClose', \n",
    "                       'Ticker','Month', 'DAX', 'ADL', 'Type1', 'Type2', 'Type3', 'Y']]\n",
    "    print(temp.Ticker.unique())\n",
    "    X = pd.concat([X, temp], axis=0, ignore_index=True, copy=True, sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = X.Ticker.unique()\n",
    "tickdict = dict(zip(T, range(len(T))))\n",
    "for tick in T:\n",
    "    X.loc[X.Ticker==tick,'Ticker']= tickdict[tick]\n",
    "    \n",
    "X.sort_values(by=['Date', 'Ticker'], inplace=True)\n",
    "X.set_index('Date', drop=True, inplace=True)\n",
    "X = X.loc[((X.index>=pd.to_datetime(d0)) & (X.index<=pd.to_datetime(d3))), :]\n",
    "\n",
    "## X is generated\n",
    "\n",
    "## Normalizing features\n",
    "C = ['EMA10', 'EMA16', 'EMA22', 'SMA10', 'SMA16', 'SMA22','ValueAtRisk', \n",
    "                       'Bollu20', 'Bollu26', 'Bollu32', 'Bolld20', 'Bolld26', 'Bolld32',\n",
    "                       'Mom12', 'Mom18', 'Mom24', 'ACC12', 'ACC18', 'ACC24', 'ROC10', 'ROC16',\n",
    "                       'ROC22', 'MACD1812', 'MACD2412','MACD3012', 'MACDS18129', 'MACDS24129', 'MACDS30129', \n",
    "                       'RSI8', 'RSI14', 'RSI20', 'OBV', 'CHV1010', 'CHV1016', 'CHV1022',\n",
    "                       'FastK12', 'FastD12', 'FastK18', 'SlowK12', 'FastD18', 'SlowD12',\n",
    "                       'FastK24', 'SlowK18', 'FastD24', 'SlowD18', 'SlowK24', 'SlowD24',\n",
    "                       'High', 'Low', 'Open', 'Close', 'Volume', 'AdjClose', 'Ticker',\n",
    "                       'Month', 'DAX', 'ADL','Type1', 'Type2', 'Type3']\n",
    "\n",
    "Cnorm = ['EMA10', 'EMA16', 'EMA22', 'SMA10', 'SMA16', 'SMA22','ValueAtRisk', \n",
    "                       'Bollu20', 'Bollu26', 'Bollu32', 'Bolld20', 'Bolld26', 'Bolld32',\n",
    "                       'Mom12', 'Mom18', 'Mom24', 'ACC12', 'ACC18', 'ACC24', 'ROC10', 'ROC16',\n",
    "                       'ROC22', 'MACD1812', 'MACD2412','MACD3012', 'MACDS18129', 'MACDS24129', 'MACDS30129', \n",
    "                       'RSI8', 'RSI14', 'RSI20', 'OBV', 'CHV1010', 'CHV1016', 'CHV1022',\n",
    "                       'FastK12', 'FastD12', 'FastK18', 'SlowK12', 'FastD18', 'SlowD12',\n",
    "                       'FastK24', 'SlowK18', 'FastD24', 'SlowD18', 'SlowK24', 'SlowD24',\n",
    "                       'High', 'Low', 'Open', 'Close', 'Volume', 'AdjClose',\n",
    "                       'Month', 'DAX', 'ADL']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "q1, q2 = X['Y'].quantile(0.98), X['Y'].quantile(0.02)\n",
    "X = X[(X['Y'] < q1) & (X['Y'] > q2)]\n",
    "\n",
    "## Normalization\n",
    "Xtrain1 = X.loc[((X.index>pd.to_datetime(d0)) & (X.index<=pd.to_datetime(d1))), C].copy()\n",
    "Ytrain1 = X.loc[((X.index>pd.to_datetime(d0)) & (X.index<=pd.to_datetime(d1))), 'Y'].copy()\n",
    "\n",
    "Xmean = np.mean(Xtrain1.loc[:,Cnorm])\n",
    "Xstdev = np.std(Xtrain1.loc[:,Cnorm])\n",
    "\n",
    "\n",
    "\n",
    "X_old = X.copy()\n",
    "X.loc[:, Cnorm] = (X.loc[:, Cnorm]-Xmean)/(Xstdev)\n",
    "\n",
    "Xcv1 = X.loc[((X.index>pd.to_datetime(d0)) & (X.index<=pd.to_datetime(d1))), C].copy()\n",
    "Ycv1 = X.loc[((X.index>pd.to_datetime(d0)) & (X.index<=pd.to_datetime(d1))), 'Y'].copy()\n",
    "Xtrain1 = X.loc[((X.index>pd.to_datetime(d1)) & (X.index<=pd.to_datetime(d2))), C].copy()\n",
    "Ytrain1 = X.loc[((X.index>pd.to_datetime(d1)) & (X.index<=pd.to_datetime(d2))), 'Y'].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drp = False\n",
    "drpProb = 0.1\n",
    "batch = False\n",
    "optimizer = 'SGD'\n",
    "learning_rate = 0.02\n",
    "n_epochs = 1000\n",
    "\n",
    "\n",
    "layer_ls = list(range(1,8))\n",
    "neuron_ls = list(range(1,16))\n",
    "\n",
    "\n",
    "loss_array = np.zeros((len(layer_ls), len(neuron_ls)))\n",
    "for nlayers in layer_ls:\n",
    "    for nneurons in neuron_ls:\n",
    "        ls = [nneurons]*nlayers\n",
    "        ls = np.insert(ls, 0, Xcv1.shape[1]).tolist()\n",
    "        ls.append(1)\n",
    "        nnt, err = npy.nnTrain(Xtrain1, Ytrain1, ls, drp, drpProb, batch, opt, learning_rate, n_epochs) \n",
    "        loss_array[nlayers-1,nneurons-1]  = npy.nnTest(nnt, Xcv1, Ycv1)        \n",
    "        plt.plot(err)        \n",
    "\n",
    "plt.xlabel('Reps')\n",
    "plt.ylabel('Losses')\n",
    "plt.title('Computed train losses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see for each network, the loss converges before 1000 training epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "\n",
    "loss_df = pd.DataFrame(loss_array, index=layer_ls, columns=neuron_ls)\n",
    "loss_df\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "neuron_array, layer_array = np.meshgrid(neuron_ls, layer_ls)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(layer_array, neuron_array, loss_array, cmap=cm.coolwarm)\n",
    "ax.set_xlabel('layers')\n",
    "ax.set_ylabel('neurons')\n",
    "ax.set_zlabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When increasing nlayers and nneurons do not improve the performance too much, we choose not to add too much complexity to the model. The selected number of layers is 4, and number of neurons per layer is 12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the train errors converge on a level far lower the testing error. This is a sign of overfitting, so we try dropout. In addition, the optimization may fall into local minimum, so we try RMSprop and Adam. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To control the covariate shift in the financial data, we also apply batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {'drp_ls': [True, False],\n",
    "             'drpProb_ls': [0.3, 0.5 ,0.6, 0.7, 0.9],\n",
    "             'batch_ls': [True, False],\n",
    "             'optimizer_ls': ['SGD', 'RMSprop', 'ADAM'],\n",
    "             'layer_ls': [2,4,6],\n",
    "              'neuron_ls': [5,7,9,11]}\n",
    "\n",
    "\n",
    "\n",
    "learning_rate = 0.02\n",
    "n_epochs = 1000\n",
    "\n",
    "\n",
    "\n",
    "result = {}\n",
    "result2 = {}\n",
    "for drp in param_dist['drp_ls']:\n",
    "    for drpProb in param_dist['drpProb_ls']:\n",
    "        for batch in param_dist['batch_ls']:\n",
    "            for optimizer in param_dist['optimizer_ls']:\n",
    "                for nlayer in param_dist['layer_ls']:\n",
    "                    for nneurons in param_dist['neuron_ls']:\n",
    "                        ls = [nneurons]*nlayers\n",
    "                        ls = np.insert(ls, 0, Xcv1.shape[1]).tolist()\n",
    "                        ls.append(1)\n",
    "                        nnt, err = npy.nnTrain(Xtrain1, Ytrain1, ls, drp, drpProb, batch, opt, learning_rate, n_epochs) \n",
    "                        string = 'drp:' + str(drp) + ' dropProb:' + str(drpProb) + ' batch:' + str(batch)\\\n",
    "                        + ' optimizer:' + str(optimizer) + ' nlayer:' + str(nlayer) + ' nneurons:' + str(nneurons)\n",
    "                        result[string]  = npy.nnTest(nnt, Xcv1, Ycv1) \n",
    "                        result2[string] = err\n",
    "                        if sum(err>20) == 0:\n",
    "                            plt.plot(err)\n",
    "                        print(string, result[string])\n",
    "\n",
    "plt.xlabel('Reps')\n",
    "plt.ylabel('Losses')\n",
    "plt.title('Computed train losses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {'drp_ls': [True, False],\n",
    "             'drpProb_ls': [0.3, 0.5 ,0.6, 0.7, 0.9],\n",
    "             'batch_ls': [True, False],\n",
    "             'optimizer_ls': ['SGD', 'RMSprop', 'ADAM'],\n",
    "             'layer_ls': [2,4,6],\n",
    "              'neuron_ls': [5,7,9,11]}\n",
    "\n",
    "\n",
    "\n",
    "learning_rate = 0.02\n",
    "n_epochs = 1000\n",
    "\n",
    "\n",
    "\n",
    "result = {}\n",
    "result2 = {}\n",
    "for drp in param_dist['drp_ls']:\n",
    "    for drpProb in param_dist['drpProb_ls']:\n",
    "        for batch in param_dist['batch_ls']:\n",
    "            for optimizer in param_dist['optimizer_ls']:\n",
    "                for nlayer in param_dist['layer_ls']:\n",
    "                    for nneurons in param_dist['neuron_ls']:\n",
    "                        ls = [nneurons]*nlayers\n",
    "                        ls = np.insert(ls, 0, Xcv1.shape[1]).tolist()\n",
    "                        ls.append(1)\n",
    "                        nnt, err = npy.nnTrain(Xtrain1, Ytrain1, ls, drp, drpProb, batch, opt, learning_rate, n_epochs) \n",
    "                        string = 'drp:' + str(drp) + ' dropProb:' + str(drpProb) + ' batch:' + str(batch)\\\n",
    "                        + ' optimizer:' + str(optimizer) + ' nlayer:' + str(nlayer) + ' nneurons:' + str(nneurons)\n",
    "                        result[string]  = npy.nnTest(nnt, Xcv1, Ycv1) \n",
    "                        result2[string] = err\n",
    "                        if sum(np.array(err)>20) == 0:\n",
    "                            plt.plot(err)\n",
    "                        print(string, result[string])\n",
    "\n",
    "plt.xlabel('Reps')\n",
    "plt.ylabel('Losses')\n",
    "plt.title('Computed train losses')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
